{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabula Muris Ingest\n",
    "\n",
    "Ingest, wrangle and store [Tabula Muris](http://tabula-muris.ds.czbiohub.org/) into a [TFRecordDataset](https://docs.google.com/presentation/d/16kHNtQslt-yuJ3w8GIx-eEH6t_AvFeQOchqGRFpAD7U/edit) optimized for multi-GPU distributed machine learning. See [github](https://github.com/czbiohub/tabula-muris) for details on the data. \n",
    "\n",
    "Note: Initially only ingesting FACS dataset as droplet has significant QC dropout and several tissues missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "\n",
    "Download the dataset and unpack into a local directory. This could be /tmp if we only plan to use the S3 copy. I usually have ~/data symlinked to a local scratch disk location for speed and to keep home directories on the shared big memory servers down in size. For a much larger dataset with the files separately downloadable it would be better to download directly into memory, convert to tfrecord and push to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to data directory for storing download and tfrecords\n",
    "!mkdir -p ~/data/tabula-muris\n",
    "os.chdir(os.path.expanduser(\"~/data/tabula-muris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all local files so we start clean\n",
    "# !rm -rf *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-10 21:24:21--  https://ndownloader.figshare.com/articles/5715040/versions/1\n",
      "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 34.255.241.113, 54.72.206.99, 34.248.163.7, ...\n",
      "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|34.255.241.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 308503289 (294M) [application/zip]\n",
      "Saving to: 'data.zip'\n",
      "\n",
      "data.zip            100%[===================>] 294.21M  9.68MB/s    in 32s     \n",
      "\n",
      "2018-10-10 21:24:54 (9.12 MB/s) - 'data.zip' saved [308503289/308503289]\n",
      "\n",
      "Archive:  data.zip\n",
      " extracting: FACS.zip                \n",
      " extracting: metadata_FACS.csv       \n",
      " extracting: annotations_FACS.csv    \n",
      "Archive:  FACS.zip\n",
      "   creating: FACS/\n",
      "  inflating: FACS/Lung-counts.csv    \n",
      "  inflating: FACS/Spleen-counts.csv  \n",
      "  inflating: FACS/Trachea-counts.csv  \n",
      "  inflating: FACS/Brain_Microglia-counts.csv  \n",
      "  inflating: FACS/Kidney-counts.csv  \n",
      "  inflating: FACS/Pancreas-counts.csv  \n",
      "  inflating: FACS/Skin-counts.csv    \n",
      "  inflating: FACS/Tongue-counts.csv  \n",
      "  inflating: FACS/Colon-counts.csv   \n",
      "  inflating: FACS/Thymus-counts.csv  \n",
      "  inflating: FACS/Liver-counts.csv   \n",
      "  inflating: FACS/Brain_Neurons-counts.csv  \n",
      "  inflating: FACS/Marrow-counts.csv  \n",
      "  inflating: FACS/Bladder-counts.csv  \n",
      "  inflating: FACS/Fat-counts.csv     \n",
      "  inflating: FACS/Muscle-counts.csv  \n",
      "  inflating: FACS/Heart-counts.csv   \n",
      "  inflating: FACS/Mammary-counts.csv  \n"
     ]
    }
   ],
   "source": [
    "# Download the FACS data set and unzip into per tissue expression csv files \n",
    "!wget --no-clobber --output-document data.zip https://ndownloader.figshare.com/articles/5715040/versions/1\n",
    "!if [ ! -f FACS.zip ]; then unzip data.zip; fi\n",
    "!if [ ! -d FACS ]; then unzip FACS.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 tissues: ['Bladder', 'Brain_Microglia', 'Brain_Neurons', 'Colon', 'Fat', 'Heart', 'Kidney', 'Liver', 'Lung', 'Mammary', 'Marrow', 'Muscle', 'Pancreas', 'Skin', 'Spleen', 'Thymus', 'Tongue', 'Trachea']\n"
     ]
    }
   ],
   "source": [
    "tissues = sorted([re.findall(r\"FACS\\/(\\w.+?)-counts.csv\", f)[0] for f in glob.glob(\"FACS/*-counts.csv\")])\n",
    "print(\"Found {} tissues: {}\".format(len(tissues), tissues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bladder: 1638 Brain_Microglia: 4762 Brain_Neurons: 5799 Colon: 4149 Fat: 5862 Heart: 7115 Kidney: 865 Liver: 981 Lung: 1923 Mammary: 2663 Marrow: 5355 Muscle: 2102 Pancreas: 1961 Skin: 2464 Spleen: 1718 Thymus: 1580 Tongue: 1432 Trachea: 1391 \n",
      "Finished. \n",
      "Total Training Samples: 43001, \n",
      "Total Test Samples: 10759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split each tissue into train and test sets and write separately into .tfrecord files\n",
    "def write_tfrecord(writer, features, labels):\n",
    "    writer.write(tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=features.values)),\n",
    "            \"labels\": tf.train.Feature(int64_list=tf.train.Int64List(value=labels))\n",
    "        })).SerializeToString())\n",
    "\n",
    "num_train_samples = 0\n",
    "num_test_samples = 0\n",
    "for tissue in tissues:\n",
    "    print(\"{}: \".format(tissue), end=\"\")\n",
    "    # Load expression for single tissue and transpose rows=features to columns=features \n",
    "    expression = pd.read_csv(\"FACS/{}-counts.csv\".format(tissue), index_col=0) \\\n",
    "        .astype(np.int32).T.sort_index(axis=\"columns\")\n",
    "    print(expression.shape[0], end=\" \")\n",
    "    \n",
    "    train, test = train_test_split(expression, test_size=0.2)\n",
    "    num_train_samples += train.shape[0]\n",
    "    num_test_samples += test.shape[0]\n",
    "\n",
    "    # Compressed which saves significant space as this is sparse data:\n",
    "    # Bladder: csv (full) = 81M, tfrecord (train) = 118M, tfrecord.gzip (train) = 14M\n",
    "    options=tf.python_io.TFRecordOptions(\n",
    "        compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "    with tf.python_io.TFRecordWriter(\"FACS/{}.train.gzip.tfrecord\".format(tissue), \n",
    "                                     options=options) as writer:\n",
    "            for (_, features) in train.iterrows():\n",
    "                write_tfrecord(writer, features, [tissues.index(tissue)])\n",
    "                \n",
    "    with tf.python_io.TFRecordWriter(\"FACS/{}.test.gzip.tfrecord\".format(tissue), \n",
    "                                     options=options) as writer:\n",
    "            for (_, features) in test.iterrows():\n",
    "                write_tfrecord(writer, features, [tissues.index(tissue)])\n",
    "            \n",
    "# Save meta data\n",
    "import json\n",
    "with open(\"FACS/metadata.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"tissues\": tissues,\n",
    "        \"num_train_samples\": num_train_samples,\n",
    "        \"num_test_samples\": num_test_samples,\n",
    "        \"genes\": expression.columns.tolist()}))\n",
    "\n",
    "print(\"\\nFinished. \\nTotal Training Samples: {}, \\nTotal Test Samples: {}\".format(\n",
    "    num_train_samples, num_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all s3 files so we start clean\n",
    "# !aws s3 rm --recursive s3://stuartlab/tabula-muris/ --profile prp --endpoint https://s3.nautilus.optiputer.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Profile: prp Endpoint: https://s3.nautilus.optiputer.net\n",
      "FACS/Bladder.train.gzip.tfrecord\n",
      "FACS/Bladder.test.gzip.tfrecord\n",
      "FACS/Brain_Microglia.train.gzip.tfrecord\n",
      "FACS/Brain_Microglia.test.gzip.tfrecord\n",
      "FACS/Brain_Neurons.train.gzip.tfrecord\n",
      "FACS/Brain_Neurons.test.gzip.tfrecord\n",
      "FACS/Colon.train.gzip.tfrecord\n",
      "FACS/Colon.test.gzip.tfrecord\n",
      "FACS/Fat.train.gzip.tfrecord\n",
      "FACS/Fat.test.gzip.tfrecord\n",
      "FACS/Heart.train.gzip.tfrecord\n",
      "FACS/Heart.test.gzip.tfrecord\n",
      "FACS/Kidney.train.gzip.tfrecord\n",
      "FACS/Kidney.test.gzip.tfrecord\n",
      "FACS/Liver.train.gzip.tfrecord\n",
      "FACS/Liver.test.gzip.tfrecord\n",
      "FACS/Lung.train.gzip.tfrecord\n",
      "FACS/Lung.test.gzip.tfrecord\n",
      "FACS/Mammary.train.gzip.tfrecord\n",
      "FACS/Mammary.test.gzip.tfrecord\n",
      "FACS/Marrow.train.gzip.tfrecord\n",
      "FACS/Marrow.test.gzip.tfrecord\n",
      "FACS/Muscle.train.gzip.tfrecord\n",
      "FACS/Muscle.test.gzip.tfrecord\n",
      "FACS/Pancreas.train.gzip.tfrecord\n",
      "FACS/Pancreas.test.gzip.tfrecord\n",
      "FACS/Skin.train.gzip.tfrecord\n",
      "FACS/Skin.test.gzip.tfrecord\n",
      "FACS/Spleen.train.gzip.tfrecord\n",
      "FACS/Spleen.test.gzip.tfrecord\n",
      "FACS/Thymus.train.gzip.tfrecord\n",
      "FACS/Thymus.test.gzip.tfrecord\n",
      "FACS/Tongue.train.gzip.tfrecord\n",
      "FACS/Tongue.test.gzip.tfrecord\n",
      "FACS/Trachea.train.gzip.tfrecord\n",
      "FACS/Trachea.test.gzip.tfrecord\n",
      "FACS/metadata.json\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import boto3\n",
    "session = boto3.session.Session(profile_name=os.getenv(\"AWS_PROFILE\"))\n",
    "s3 = session.resource(\"s3\", endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\"))\n",
    "bucket = s3.Bucket(\"stuartlab\")\n",
    "s3_prefix = \"tabula-muris/\"\n",
    "print(\"S3 Profile: {} Endpoint: {}\".format(os.getenv(\"AWS_PROFILE\"), os.getenv(\"AWS_S3_ENDPOINT\")))\n",
    "\n",
    "# Upload the json and tfrecord files to s3. TFRecordWrite doesn't appear to support \n",
    "# writing directly to S3, otherwise we could read, wrangle and write to s3 from memory\n",
    "for path in glob.glob(\"FACS/*.tfrecord\") + glob.glob(\"FACS/*.json\"):\n",
    "    print(path)\n",
    "    bucket.Object(s3_prefix + path).upload_file(path, ExtraArgs={'ACL':'public-read'})\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-10 21:39:41    3583903 Bladder.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:40   13967144 Bladder.train.gzip.tfrecord\r\n",
      "2018-10-10 21:39:45    6161311 Brain_Microglia.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:44   24725897 Brain_Microglia.train.gzip.tfrecord\r\n",
      "2018-10-10 21:39:47    7041452 Brain_Neurons.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:47   27650697 Brain_Neurons.train.gzip.tfrecord\r\n",
      "2018-10-10 21:39:51    8393108 Colon.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:49   34024941 Colon.train.gzip.tfrecord\r\n",
      "2018-10-10 21:39:55    9121552 Fat.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:53   36525787 Fat.train.gzip.tfrecord\r\n",
      "2018-10-10 21:39:58    8753749 Heart.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:56   33654460 Heart.train.gzip.tfrecord\r\n",
      "2018-10-10 21:39:59     642001 Kidney.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:59    2553561 Kidney.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:00    1422611 Liver.test.gzip.tfrecord\r\n",
      "2018-10-10 21:39:59    5712921 Liver.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:02    2831570 Lung.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:01   10738675 Lung.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:04    5064337 Mammary.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:04   19719503 Mammary.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:08    8880758 Marrow.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:06   36911896 Marrow.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:10    2847907 Muscle.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:10   11258002 Muscle.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:13    3676026 Pancreas.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:12   14650535 Pancreas.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:16    4511908 Skin.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:16   17939004 Skin.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:17    2005436 Spleen.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:17    8088239 Spleen.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:18    1754143 Thymus.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:18    7192673 Thymus.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:20    3516066 Tongue.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:20   13958749 Tongue.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:22    2609287 Trachea.test.gzip.tfrecord\r\n",
      "2018-10-10 21:40:21   10444898 Trachea.train.gzip.tfrecord\r\n",
      "2018-10-10 21:40:22     244525 metadata.json\r\n"
     ]
    }
   ],
   "source": [
    "!aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "    s3 ls s3://stuartlab/tabula-muris/FACS/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
