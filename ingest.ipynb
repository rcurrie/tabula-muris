{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabula Muris Ingest\n",
    "\n",
    "Ingest, wrangle and store [Tabula Muris](http://tabula-muris.ds.czbiohub.org/) into a [TFRecordDataset](https://docs.google.com/presentation/d/16kHNtQslt-yuJ3w8GIx-eEH6t_AvFeQOchqGRFpAD7U/edit) optimized for multi-GPU distributed machine learning. See [github](https://github.com/czbiohub/tabula-muris) for details on the data. \n",
    "\n",
    "Note: Initially only ingesting FACS dataset as droplet has significant QC dropout and several tissues missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "\n",
    "Download the dataset and unpack into a local directory. This could be /tmp if we only plan to use the S3 copy. I usually have ~/data symlinked to a local scratch disk location for speed and to keep home directories on the shared big memory servers down in size. For a much larger dataset with the files separately downloadable it would be better to download directly into memory, convert to tfrecord and push to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to data directory for storing download and tfrecords\n",
    "!mkdir -p ~/data/tabula-muris\n",
    "os.chdir(os.path.expanduser(\"~/data/tabula-muris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-11 02:46:25--  https://ndownloader.figshare.com/articles/5715040/versions/1\n",
      "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 34.248.163.7, 34.250.206.8, 34.255.241.113, ...\n",
      "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|34.248.163.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 308503289 (294M) [application/zip]\n",
      "Saving to: 'data.zip'\n",
      "\n",
      "data.zip            100%[===================>] 294.21M  9.91MB/s    in 31s     \n",
      "\n",
      "2018-10-11 02:46:56 (9.46 MB/s) - 'data.zip' saved [308503289/308503289]\n",
      "\n",
      "Archive:  data.zip\n",
      " extracting: FACS.zip                \n",
      " extracting: metadata_FACS.csv       \n",
      " extracting: annotations_FACS.csv    \n",
      "Archive:  FACS.zip\n",
      "   creating: FACS/\n",
      "  inflating: FACS/Lung-counts.csv    \n",
      "  inflating: FACS/Spleen-counts.csv  \n",
      "  inflating: FACS/Trachea-counts.csv  \n",
      "  inflating: FACS/Brain_Microglia-counts.csv  \n",
      "  inflating: FACS/Kidney-counts.csv  \n",
      "  inflating: FACS/Pancreas-counts.csv  \n",
      "  inflating: FACS/Skin-counts.csv    \n",
      "  inflating: FACS/Tongue-counts.csv  \n",
      "  inflating: FACS/Colon-counts.csv   \n",
      "  inflating: FACS/Thymus-counts.csv  \n",
      "  inflating: FACS/Liver-counts.csv   \n",
      "  inflating: FACS/Brain_Neurons-counts.csv  \n",
      "  inflating: FACS/Marrow-counts.csv  \n",
      "  inflating: FACS/Bladder-counts.csv  \n",
      "  inflating: FACS/Fat-counts.csv     \n",
      "  inflating: FACS/Muscle-counts.csv  \n",
      "  inflating: FACS/Heart-counts.csv   \n",
      "  inflating: FACS/Mammary-counts.csv  \n"
     ]
    }
   ],
   "source": [
    "# Download the FACS data set and unzip into per tissue expression csv files \n",
    "!wget --no-clobber --output-document data.zip https://ndownloader.figshare.com/articles/5715040/versions/1\n",
    "!if [ ! -f FACS.zip ]; then unzip data.zip; fi\n",
    "!if [ ! -d FACS ]; then unzip FACS.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 tissues: ['Bladder', 'Brain_Microglia', 'Brain_Neurons', 'Colon', 'Fat', 'Heart', 'Kidney', 'Liver', 'Lung', 'Mammary', 'Marrow', 'Muscle', 'Pancreas', 'Skin', 'Spleen', 'Thymus', 'Tongue', 'Trachea']\n"
     ]
    }
   ],
   "source": [
    "tissues = sorted([re.findall(r\"FACS\\/(\\w.+?)-counts.csv\", f)[0] for f in glob.glob(\"FACS/*-counts.csv\")])\n",
    "print(\"Found {} tissues: {}\".format(len(tissues), tissues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle\n",
    "Divide each tissue.csv into train and test sets and store into .tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bladder: 1638 Brain_Microglia: 4762 Brain_Neurons: 5799 Colon: 4149 Fat: 5862 Heart: 7115 Kidney: 865 Liver: 981 Lung: 1923 Mammary: 2663 Marrow: 5355 Muscle: 2102 Pancreas: 1961 Skin: 2464 Spleen: 1718 Thymus: 1580 Tongue: 1432 Trachea: 1391 \n",
      "Finished. \n",
      "Total Training Samples: 43001, \n",
      "Total Test Samples: 10759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def write_tfrecord(writer, features, labels):\n",
    "    writer.write(tf.train.Example(features=tf.train.Features(\n",
    "        feature = {\n",
    "            \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=features.values)),\n",
    "            \"labels\": tf.train.Feature(int64_list=tf.train.Int64List(value=labels))\n",
    "        })).SerializeToString())\n",
    "\n",
    "num_train_samples = 0\n",
    "num_test_samples = 0\n",
    "for tissue in tissues:\n",
    "    print(\"{}: \".format(tissue), end=\"\")\n",
    "    # Load expression for single tissue and transpose rows=features to columns=features \n",
    "    expression = pd.read_csv(\"FACS/{}-counts.csv\".format(tissue), index_col=0) \\\n",
    "        .astype(np.int32).T.sort_index(axis=\"columns\")\n",
    "    print(expression.shape[0], end=\" \")\n",
    "    \n",
    "    train, test = train_test_split(expression, test_size=0.2)\n",
    "    num_train_samples += train.shape[0]\n",
    "    num_test_samples += test.shape[0]\n",
    "\n",
    "    # Compressed which saves significant space as this is sparse data:\n",
    "    # Bladder: csv (full) = 81M, tfrecord (train) = 118M, tfrecord.gzip (train) = 14M\n",
    "    options=tf.python_io.TFRecordOptions(\n",
    "        compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "    with tf.python_io.TFRecordWriter(\"FACS/{}.train.gzip.tfrecord\".format(tissue), \n",
    "                                     options=options) as writer:\n",
    "            for (_, features) in train.iterrows():\n",
    "                write_tfrecord(writer, features, [tissues.index(tissue)])\n",
    "                \n",
    "    with tf.python_io.TFRecordWriter(\"FACS/{}.test.gzip.tfrecord\".format(tissue), \n",
    "                                     options=options) as writer:\n",
    "            for (_, features) in test.iterrows():\n",
    "                write_tfrecord(writer, features, [tissues.index(tissue)])\n",
    "            \n",
    "# Save meta data\n",
    "import json\n",
    "with open(\"FACS/metadata.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"tissues\": tissues,\n",
    "        \"num_train_samples\": num_train_samples,\n",
    "        \"num_test_samples\": num_test_samples,\n",
    "        \"genes\": expression.columns.tolist()}))\n",
    "\n",
    "print(\"\\nFinished. \\nTotal Training Samples: {}, \\nTotal Test Samples: {}\".format(\n",
    "    num_train_samples, num_test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload\n",
    "Upload the tfrecord and metadata into S3 so we can train on it from pods in the k8s cluster. We could move this into the tfrecord generation above for a large dataset to reduce the amount stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all s3 files so we start clean\n",
    "# !aws s3 rm --recursive s3://stuartlab/tabula-muris/ --profile prp --endpoint https://s3.nautilus.optiputer.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: FACS/Bladder.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Bladder.test.gzip.tfrecord\n",
      "upload: FACS/Brain_Microglia.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Brain_Microglia.test.gzip.tfrecord\n",
      "upload: FACS/Brain_Neurons.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Brain_Neurons.test.gzip.tfrecord\n",
      "upload: FACS/Brain_Microglia.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Brain_Microglia.train.gzip.tfrecord\n",
      "upload: FACS/Bladder.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Bladder.train.gzip.tfrecord\n",
      "upload: FACS/Colon.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Colon.test.gzip.tfrecord\n",
      "upload: FACS/Brain_Neurons.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Brain_Neurons.train.gzip.tfrecord\n",
      "upload: FACS/Kidney.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Kidney.train.gzip.tfrecord\n",
      "upload: FACS/Liver.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Liver.test.gzip.tfrecord\n",
      "upload: FACS/Kidney.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Kidney.test.gzip.tfrecord\n",
      "upload: FACS/Fat.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Fat.test.gzip.tfrecord\n",
      "upload: FACS/Heart.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Heart.test.gzip.tfrecord\n",
      "upload: FACS/Colon.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Colon.train.gzip.tfrecord\n",
      "upload: FACS/Lung.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Lung.test.gzip.tfrecord\n",
      "upload: FACS/Liver.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Liver.train.gzip.tfrecord\n",
      "upload: FACS/Mammary.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Mammary.test.gzip.tfrecord\n",
      "upload: FACS/Fat.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Fat.train.gzip.tfrecord\n",
      "upload: FACS/Muscle.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Muscle.test.gzip.tfrecord\n",
      "upload: FACS/Heart.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Heart.train.gzip.tfrecord\n",
      "upload: FACS/Lung.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Lung.train.gzip.tfrecord\n",
      "upload: FACS/Pancreas.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Pancreas.test.gzip.tfrecord\n",
      "upload: FACS/Marrow.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Marrow.test.gzip.tfrecord\n",
      "upload: FACS/Skin.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Skin.test.gzip.tfrecord\n",
      "upload: FACS/Spleen.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Spleen.test.gzip.tfrecord\n",
      "upload: FACS/Mammary.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Mammary.train.gzip.tfrecord\n",
      "upload: FACS/Thymus.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Thymus.test.gzip.tfrecord\n",
      "upload: FACS/Spleen.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Spleen.train.gzip.tfrecord\n",
      "upload: FACS/Muscle.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Muscle.train.gzip.tfrecord\n",
      "upload: FACS/Thymus.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Thymus.train.gzip.tfrecord\n",
      "upload: FACS/Tongue.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Tongue.test.gzip.tfrecord\n",
      "upload: FACS/Trachea.test.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Trachea.test.gzip.tfrecord\n",
      "upload: FACS/Marrow.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Marrow.train.gzip.tfrecord\n",
      "upload: FACS/metadata.json to s3://stuartlab/tabula-muris/dataset/metadata.json\n",
      "upload: FACS/Pancreas.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Pancreas.train.gzip.tfrecord\n",
      "upload: FACS/Skin.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Skin.train.gzip.tfrecord\n",
      "upload: FACS/Trachea.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Trachea.train.gzip.tfrecord\n",
      "upload: FACS/Tongue.train.gzip.tfrecord to s3://stuartlab/tabula-muris/dataset/Tongue.train.gzip.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# Use the aws cli's rsync like sync command\n",
    "!aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "    s3 sync FACS/ s3://stuartlab/tabula-muris/dataset --exclude \"*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy each file via boto3...preserved for reference when/if we overlap with tfrecord creation above\n",
    "# import glob\n",
    "# import boto3\n",
    "\n",
    "# bucket_name = \"stuartlab\"\n",
    "# project_name = \"tabula-muris\"  # Dataset folder and output location\n",
    "\n",
    "# session = boto3.session.Session(profile_name=os.getenv(\"AWS_PROFILE\"))\n",
    "# s3 = session.resource(\"s3\", endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\"))\n",
    "# bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# # Upload the json and tfrecord files to s3. TFRecordWrite doesn't appear to support \n",
    "# # writing directly to S3, otherwise we could read, wrangle and write to s3 from memory\n",
    "# for path in glob.glob(\"FACS/*.tfrecord\") + glob.glob(\"FACS/*.json\"):\n",
    "#     print(path)\n",
    "#     bucket.Object(\"{}/{}\".format(project, path)).upload_file(path, ExtraArgs={'ACL':'public-read'})\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-11 03:04:32    3601253 Bladder.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:34   13949030 Bladder.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:33    6087427 Brain_Microglia.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:34   24800122 Brain_Microglia.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:33    6953732 Brain_Neurons.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:35   27735153 Brain_Neurons.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:34    8509884 Colon.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:37   33913093 Colon.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:37    9294719 Fat.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:38   36352355 Fat.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:37    8437405 Heart.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:40   33972222 Heart.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:37     656415 Kidney.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:37    2540177 Kidney.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:37    1459875 Liver.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:38    5672397 Liver.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:38    2785052 Lung.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:40   10785135 Lung.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:39    5038144 Mammary.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:42   19744609 Mammary.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:41    9326543 Marrow.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:44   36469479 Marrow.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:40    2703489 Muscle.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:43   11402492 Muscle.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:41    3632010 Pancreas.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:45   14696136 Pancreas.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:41    4469012 Skin.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:46   17984716 Skin.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:42    2041306 Spleen.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:44    8051669 Spleen.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:43    1742890 Thymus.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:44    7203103 Thymus.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:44    3466780 Tongue.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:47   14009235 Tongue.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:45    2633538 Trachea.test.gzip.tfrecord\r\n",
      "2018-10-11 03:04:48   10418991 Trachea.train.gzip.tfrecord\r\n",
      "2018-10-11 03:04:45     244525 metadata.json\r\n"
     ]
    }
   ],
   "source": [
    "!aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "    s3 ls s3://stuartlab/tabula-muris/dataset/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
