{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabula Muris Train\n",
    "\n",
    "Train a tissue classifier on [Tabula Muris](http://tabula-muris.ds.czbiohub.org/) as per the paper:\n",
    "\n",
    "[Learning a hierarchical representation of the yeast transcriptomic machinery using an autoencoder model](https://www.ncbi.nlm.nih.gov/pubmed/26818848)\n",
    "\n",
    "Other References:\n",
    "\n",
    "http://tiao.io/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/\n",
    "\n",
    "Read and write exclusively using S3 so that this notebook converted to python can be run in a kubernettes cluster for distributed machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG ON\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)  # reproducibility\n",
    "\n",
    "# Simple syntatic sugar for debug vs. train parameters\n",
    "def debug(debug_param, no_debug_param):\n",
    "    return debug_param if os.environ.get(\"DEBUG\") else no_debug_param\n",
    "print(debug(\"DEBUG ON\", \"DEBUG OFF\"))\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "bucket_name = \"stuartlab\"\n",
    "project_name = \"tabula-muris\"  # Dataset folder and output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Profile: prp Endpoint: https://s3.nautilus.optiputer.net Project: tabula-muris\n",
      "Dataset metadata keys: ['num_test_samples', 'num_train_samples', 'genes', 'tissues']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "session = boto3.session.Session(profile_name=os.getenv(\"AWS_PROFILE\"))\n",
    "bucket = session.resource(\n",
    "    \"s3\", endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\")).Bucket(bucket_name)\n",
    "print(\"S3 Profile: {} Endpoint: {} Project: {}\".format(\n",
    "    os.getenv(\"AWS_PROFILE\"), os.getenv(\"AWS_S3_ENDPOINT\"), project_name))\n",
    "\n",
    "metadata = json.loads(bucket.Object(\n",
    "    project_name + \"/FACS/metadata.json\").get()['Body'].read().decode('utf-8'))\n",
    "print(\"Dataset metadata keys:\", list(metadata.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model params - we'll store final hyper parameters here as well\n",
    "params = {\"batch_size\": 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 training tfrecord files\n",
      "Found 18 testing tfrecord files\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset that supports distributed training efficiently\n",
    "def parse_one_example(example):\n",
    "    features = {\n",
    "        \"features\": tf.FixedLenFeature([len(metadata[\"genes\"])], tf.float32),\n",
    "        \"labels\": tf.FixedLenFeature([], tf.int64)\n",
    "    }          \n",
    "    example = tf.parse_single_example(example, features)\n",
    "    return example[\"features\"], tf.one_hot(example[\"labels\"], len(metadata[\"tissues\"]))\n",
    "\n",
    "def create_dataset(files, batch_size, num_classes):\n",
    "    # Assuming one file per class we set the cycle length to the\n",
    "    # total number of classes so that our batches stratify accross all\n",
    "    # classess and then shuffle\n",
    "    files = tf.data.Dataset.list_files(files)\n",
    "    dataset = files.interleave(lambda x: tf.data.TFRecordDataset(x, compression_type=\"GZIP\").prefetch(64), \n",
    "                               cycle_length=num_classes, block_length=16*num_classes)\n",
    "\n",
    "    # Use num_parallel_calls to parallelize map().\n",
    "    dataset = dataset.map(parse_one_example, num_parallel_calls=num_classes)\n",
    "    dataset = dataset.shuffle(16*num_classes)\n",
    "    dataset = dataset.repeat(100)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Use prefetch() to overlap the producer and consumer.\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "# Find all the training files\n",
    "training_files = [\"s3://{}/{}\".format(bucket_name, o.key) \n",
    "         for o in bucket.objects.filter(Prefix=project_name + \"/FACS/\") \n",
    "         if o.key.endswith(\"train.gzip.tfrecord\")]\n",
    "print(\"Found {} training tfrecord files\".format(len(training_files)))\n",
    "training_dataset = create_dataset(training_files, params[\"batch_size\"], len(training_files))\n",
    "\n",
    "# Find all the test files\n",
    "testing_files = [\"s3://{}/{}\".format(bucket_name, o.key) \n",
    "         for o in bucket.objects.filter(Prefix=project_name + \"/FACS/\") \n",
    "         if o.key.endswith(\"test.gzip.tfrecord\")]\n",
    "print(\"Found {} testing tfrecord files\".format(len(testing_files)))\n",
    "test_dataset = create_dataset(testing_files, params[\"batch_size\"], len(testing_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 23433)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 23433)             93732     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1499776   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 1,598,838\n",
      "Trainable params: 1,551,972\n",
      "Non-trainable params: 46,866\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 5s 508ms/step - loss: 4.2842 - acc: 0.1180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a39751940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(input_size, output_size, hyperparameters={\"width\": 64, \"depth\": 2, \"penalty\": 1e-5}):\n",
    "    input_layer = tf.keras.Input(shape=(input_size, ), name=\"input_layer\")\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(input_layer)\n",
    "        \n",
    "    for i in range(hyperparameters[\"depth\"]):\n",
    "        x = tf.keras.layers.Dense(hyperparameters[\"width\"],\n",
    "                                  activity_regularizer=tf.keras.regularizers.l1(\n",
    "                                     hyperparameters[\"penalty\"]), activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(output_size, activation=\"softmax\")(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model = create_model(input_size=len(metadata[\"genes\"]), output_size=len(metadata[\"tissues\"]))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=tf.contrib.keras.optimizers.Adam())\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(training_dataset, epochs=debug(1, 10),\n",
    "          steps_per_epoch=debug(10, metadata[\"num_train_samples\"] // params[\"batch_size\"]),\n",
    "          callbacks=[tf.keras.callbacks.TensorBoard(log_dir=\"/tmp\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 436ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[29.101637601852417, 0.2640625]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset, \n",
    "               steps=debug(10, metadata[\"num_test_samples\"] // params[\"batch_size\"]))                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all s3 files so we start clean\n",
    "# !aws s3 rm --recursive s3://stuartlab/tabula-muris/models --profile prp --endpoint https://s3.nautilus.optiputer.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the params, model and weights to S3 for evaluation back on jupyter\n",
    "bucket.Object(\"{0}/models/{0}.params.json\".format(project_name)).put(Body=json.dumps(params), ACL=\"public-read\")\n",
    "\n",
    "import tempfile\n",
    "tempname = next(tempfile._get_candidate_names())\n",
    "model.save(\"/tmp/{}.h5\".format(tempname))\n",
    "bucket.Object(\"{0}/models/{0}.model.h5\".format(project_name)).upload_file(\n",
    "    \"/tmp/{}.h5\".format(tempname), ExtraArgs={\"ACL\":\"public-read\"})\n",
    "os.remove(\"/tmp/{}.h5\".format(tempname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-10 21:44:13   18847136 tabula-muris.model.h5\r\n",
      "2018-10-10 21:44:12         19 tabula-muris.params.json\r\n"
     ]
    }
   ],
   "source": [
    "# !aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "#     s3 ls s3://stuartlab/tabula-muris/models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
